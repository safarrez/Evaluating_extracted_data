{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db8e056a",
   "metadata": {},
   "source": [
    "# Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3413cce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 \n",
    "import os\n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d1cc4d",
   "metadata": {},
   "source": [
    "### \"Pre processing\" the data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098aa8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f4e312",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('content\\ground_truth\\data.xlsx')\n",
    "ground_truth_df = df[['Study_ID', 'Study', 'Allocation', 'Experimenter']]\n",
    "ground_truth_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ae5ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting data from the csv files in the extracted directory\n",
    "\n",
    "extracted_dir = 'content/extracted'\n",
    "extracted_dfs = []\n",
    "\n",
    "#def extract_data_from_csv(extracted_dir, extracted_dfs):\n",
    "if os.path.exists(extracted_dir):\n",
    "    for filename in os.listdir(extracted_dir):\n",
    "        if filename.endswith('.csv'):\n",
    "            file_path = os.path.join(extracted_dir, filename)\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                extracted_dfs.append(df)\n",
    "                print(f\"Loaded {filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load {filename}: {e}\")\n",
    "\n",
    "if extracted_dfs:\n",
    "    extracted_combined_df = pd.concat(extracted_dfs, ignore_index=True)\n",
    "    print(\"Combined DataFrame created.\")\n",
    "else:\n",
    "    print(\"No CSV files found in the directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "344ac59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_from_csv(extracted_dir):\n",
    "    \"\"\"\n",
    "    Loads all CSV files from the given directory into a combined DataFrame.\n",
    "\n",
    "    Args:\n",
    "        extracted_dir (str): Path to the directory containing CSV files.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Combined DataFrame of all loaded CSV files.\n",
    "    \"\"\"\n",
    "    print(f\"Extracting data from CSV files in directory: {extracted_dir}\")\n",
    "    if os.path.exists(extracted_dir):\n",
    "        for filename in os.listdir(extracted_dir):\n",
    "            if filename.endswith('.csv'):\n",
    "                file_path = os.path.join(extracted_dir, filename)\n",
    "                try:\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    extracted_dfs.append(df)\n",
    "                    #print(f\"Loaded {filename}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to load {filename}: {e}\")\n",
    "\n",
    "    if extracted_dfs:\n",
    "        extracted_combined_df = pd.concat(extracted_dfs, ignore_index=True)\n",
    "        print(\"Combined DataFrame created.\")\n",
    "        print(f\"Total rows in combined DataFrame: {len(extracted_combined_df)}\")\n",
    "        print(\"-\"* 40)\n",
    "        return extracted_combined_df\n",
    "    else:\n",
    "        print(\"No CSV files found in the directory.\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "da40d713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data from CSV files in directory: content/extracted\n",
      "Combined DataFrame created.\n",
      "Total rows in combined DataFrame: 472\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "extracted_combined_df = extract_data_from_csv(extracted_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8decfc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data from CSV files in directory: content\\extracted_split_1_shuffle_1\n",
      "Combined DataFrame created.\n",
      "Total rows in combined DataFrame: 339\n",
      "----------------------------------------\n",
      "Extracting data from CSV files in directory: content\\extracted_split_1_shuffle_2\n",
      "Combined DataFrame created.\n",
      "Total rows in combined DataFrame: 357\n",
      "----------------------------------------\n",
      "Extracting data from CSV files in directory: content\\extracted_split_2_shuffle_1\n",
      "Combined DataFrame created.\n",
      "Total rows in combined DataFrame: 374\n",
      "----------------------------------------\n",
      "Extracting data from CSV files in directory: content\\extracted_split_2_shuffle_2\n",
      "Combined DataFrame created.\n",
      "Total rows in combined DataFrame: 391\n",
      "----------------------------------------\n",
      "Extracting data from CSV files in directory: content\\extracted_split_3_shuffle_1\n",
      "Combined DataFrame created.\n",
      "Total rows in combined DataFrame: 407\n",
      "----------------------------------------\n",
      "Extracting data from CSV files in directory: content\\extracted_split_3_shuffle_2\n",
      "Combined DataFrame created.\n",
      "Total rows in combined DataFrame: 423\n",
      "----------------------------------------\n",
      "Extracting data from CSV files in directory: content\\extracted_split_4_shuffle_1\n",
      "Combined DataFrame created.\n",
      "Total rows in combined DataFrame: 438\n",
      "----------------------------------------\n",
      "Extracting data from CSV files in directory: content\\extracted_split_4_shuffle_2\n",
      "Combined DataFrame created.\n",
      "Total rows in combined DataFrame: 453\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "ext_split1_shuffle_1 = extract_data_from_csv('content\\extracted_split_1_shuffle_1')\n",
    "ext_split1_shuffle_2 = extract_data_from_csv('content\\extracted_split_1_shuffle_2')\n",
    "ext_split2_shuffle_1 = extract_data_from_csv('content\\extracted_split_2_shuffle_1')\n",
    "ext_split2_shuffle_2 = extract_data_from_csv('content\\extracted_split_2_shuffle_2')\n",
    "ext_split3_shuffle_1 = extract_data_from_csv('content\\extracted_split_3_shuffle_1')\n",
    "ext_split3_shuffle_2 = extract_data_from_csv('content\\extracted_split_3_shuffle_2')\n",
    "ext_split4_shuffle_1 = extract_data_from_csv('content\\extracted_split_4_shuffle_1')\n",
    "ext_split4_shuffle_2 = extract_data_from_csv('content\\extracted_split_4_shuffle_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d63f2b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directly taken from Anna's code\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def remove_accents(input_str):\n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
    "    return \"\".join([c for c in nfkd_form if not unicodedata.combining(c)])\n",
    "\n",
    "def format_studyName(study_name_string):\n",
    "    \"\"\"\n",
    "    Cuts a string after the last four-digit number, assuming it represents the year.\n",
    "\n",
    "    Args:\n",
    "        study_name_string (str): The input string potentially containing a year.\n",
    "\n",
    "    Returns:\n",
    "        str: The string cut after the year, or the original string if no year is found.\n",
    "    \"\"\"\n",
    "    # Get rid of all the points, -\n",
    "    study_name_string = study_name_string.replace('.', '')\n",
    "    study_name_string = study_name_string.replace(',', '')\n",
    "    study_name_string = study_name_string.replace(' - ', ' ')\n",
    "    study_name_string = study_name_string.replace(')', '')\n",
    "    study_name_string = study_name_string.replace('(', '')\n",
    "    study_name_string = study_name_string.replace('&', 'and')\n",
    "    study_name_string = remove_accents(study_name_string)\n",
    "    # Find all occurrences of four consecutive digits (potential years)\n",
    "    year_matches = list(re.finditer(r'\\b\\d{4}\\b', study_name_string))\n",
    "\n",
    "    if year_matches:\n",
    "        # Get the last match\n",
    "        last_year_match = year_matches[-1]\n",
    "        # Get the end index of the last year match\n",
    "        end_of_year_index = last_year_match.end()\n",
    "        # Slice the string up to the end of the year\n",
    "        cut_string = study_name_string[:end_of_year_index]\n",
    "        return cut_string.strip() # Use strip to remove trailing whitespace\n",
    "    else:\n",
    "        # If no four-digit number is found, return the original string\n",
    "        return study_name_string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e9dff752",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_combined_df['Study'] = extracted_combined_df['Study'].apply(format_studyName)\n",
    "extracted_combined_df\n",
    "\n",
    "ext_split1_shuffle_1['Study'] = ext_split1_shuffle_1['Study'].apply(format_studyName)\n",
    "ext_split1_shuffle_2['Study'] = ext_split1_shuffle_2['Study'].apply(format_studyName)\n",
    "ext_split2_shuffle_1['Study'] = ext_split2_shuffle_1['Study'].apply(format_studyName)\n",
    "ext_split2_shuffle_2['Study'] = ext_split2_shuffle_2['Study'].apply(format_studyName)\n",
    "ext_split3_shuffle_1['Study'] = ext_split3_shuffle_1['Study'].apply(format_studyName)\n",
    "ext_split3_shuffle_2['Study'] = ext_split3_shuffle_2['Study'].apply(format_studyName)\n",
    "ext_split4_shuffle_1['Study'] = ext_split4_shuffle_1['Study'].apply(format_studyName)\n",
    "ext_split4_shuffle_2['Study'] = ext_split4_shuffle_2['Study'].apply(format_studyName)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76309230",
   "metadata": {},
   "source": [
    "#### To group the tables by Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "54d594df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#provided by Anna \n",
    "def accuracy_check(col_name, df_extracted, test_table):\n",
    "  allocation_match = False\n",
    "  experimenter_match = False\n",
    "\n",
    "  # Check if both dataframes have the expected columns and rows\n",
    "  if col_name in df_extracted.columns and \\\n",
    "    not df_extracted.empty and not test_table.empty:\n",
    "\n",
    "      extracted_allocation = df_extracted[col_name].iloc[0]\n",
    "\n",
    "      ground_truth_allocation = test_table[col_name].iloc[0]\n",
    "\n",
    "      # Simple case-insensitive comparison\n",
    "      if str(extracted_allocation).lower() == str(ground_truth_allocation).lower():\n",
    "          allocation_match = True\n",
    "          print(f'{col_name}: Match')\n",
    "      else:\n",
    "          print(f\"{col_name}: Mismatch (Extracted: '{extracted_allocation}', Ground Truth: '{ground_truth_allocation}')\")\n",
    "  else:\n",
    "    print(\"Cannot perform accuracy check: Extracted or ground truth data is missing or malformed.\")\n",
    "  print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "430028f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocation: Match\n",
      "--------------------\n",
      "Experimenter: Mismatch (Extracted: 'Teacher', Ground Truth: 'Parent')\n",
      "--------------------\n",
      "Allocation: Match\n",
      "--------------------\n",
      "Experimenter: Mismatch (Extracted: 'Teacher', Ground Truth: 'Parent')\n",
      "--------------------\n",
      "Allocation: Match\n",
      "--------------------\n",
      "Experimenter: Mismatch (Extracted: 'Teacher', Ground Truth: 'Parent')\n",
      "--------------------\n",
      "Allocation: Match\n",
      "--------------------\n",
      "Experimenter: Mismatch (Extracted: 'Teacher', Ground Truth: 'Parent')\n",
      "--------------------\n",
      "Allocation: Match\n",
      "--------------------\n",
      "Experimenter: Mismatch (Extracted: 'Teacher', Ground Truth: 'Parent')\n",
      "--------------------\n",
      "Allocation: Match\n",
      "--------------------\n",
      "Experimenter: Mismatch (Extracted: 'Teacher', Ground Truth: 'Parent')\n",
      "--------------------\n",
      "Allocation: Match\n",
      "--------------------\n",
      "Experimenter: Mismatch (Extracted: 'Teacher', Ground Truth: 'Parent')\n",
      "--------------------\n",
      "Allocation: Match\n",
      "--------------------\n",
      "Experimenter: Mismatch (Extracted: 'Teacher', Ground Truth: 'Parent')\n",
      "--------------------\n",
      "Allocation: Match\n",
      "--------------------\n",
      "Experimenter: Mismatch (Extracted: 'Teacher', Ground Truth: 'Parent')\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# Use the accuracy_check function to compare 'Allocation' between df and extracted_combined_df\n",
    "accuracy_check('Allocation', extracted_combined_df, ground_truth_df)\n",
    "accuracy_check('Experimenter', extracted_combined_df, ground_truth_df)\n",
    "\n",
    "accuracy_check('Allocation', ext_split1_shuffle_1, ground_truth_df)\n",
    "accuracy_check('Experimenter', ext_split1_shuffle_1, ground_truth_df)\n",
    "accuracy_check('Allocation', ext_split1_shuffle_2, ground_truth_df)\n",
    "accuracy_check('Experimenter', ext_split1_shuffle_2, ground_truth_df)\n",
    "\n",
    "accuracy_check('Allocation', ext_split2_shuffle_1, ground_truth_df)\n",
    "accuracy_check('Experimenter', ext_split2_shuffle_1, ground_truth_df)\n",
    "accuracy_check('Allocation', ext_split2_shuffle_2, ground_truth_df)\n",
    "accuracy_check('Experimenter', ext_split2_shuffle_2, ground_truth_df)\n",
    "\n",
    "accuracy_check('Allocation', ext_split3_shuffle_1, ground_truth_df)\n",
    "accuracy_check('Experimenter', ext_split3_shuffle_1, ground_truth_df)\n",
    "accuracy_check('Allocation', ext_split3_shuffle_2, ground_truth_df)\n",
    "accuracy_check('Experimenter', ext_split3_shuffle_2, ground_truth_df)\n",
    "\n",
    "accuracy_check('Allocation', ext_split4_shuffle_1, ground_truth_df)\n",
    "accuracy_check('Experimenter', ext_split4_shuffle_1, ground_truth_df)\n",
    "accuracy_check('Allocation', ext_split4_shuffle_2, ground_truth_df)\n",
    "accuracy_check('Experimenter', ext_split4_shuffle_2, ground_truth_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b44c39e",
   "metadata": {},
   "source": [
    "## Calculating the metrics\n",
    "Auxilary functions used to calculate the metrics just so it can be easier to read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb0f434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate various metrics based on true positives, false positives, and false negatives\n",
    "def calculateAccuracy(TP, FP, FN):\n",
    "    '''calculates the accuracy of a model based on true positives, true negatives, false positives, and false negatives.'''\n",
    "    return float(TP + / (TP + FP + FN) if (TP + FP + FN) > 0 else 0)\n",
    "\n",
    "def calculatePrecision(TP, FP):\n",
    "    '''calculates the precision of a model based on true positives and false positives.'''\n",
    "    return float(TP / (TP + FP) if (TP + FP) > 0 else 0)\n",
    "\n",
    "def calculateRecall(TP, FN):\n",
    "    '''calculates the recall of a model based on true positives and false negatives.'''\n",
    "    return float(TP / (TP + FN) if (TP + FN) > 0 else 0)\n",
    "\n",
    "def calculateF1Score(precision, recall):\n",
    "    '''calculates the F1 score based on precision and recall.'''\n",
    "    return float(2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0)\n",
    "\n",
    "def calculateMetrics(TP, FP, FN):\n",
    "    '''calculates various metrics based on true positives, false positives, and false negatives.'''\n",
    "    # --- Metrics ---\n",
    "    accuracy = calculateAccuracy(TP, FP, FN) \n",
    "    recall = calculateRecall(TP, FN)\n",
    "    precision = calculatePrecision(TP, FP)\n",
    "    f1 = calculateF1Score(precision, recall)\n",
    "\n",
    "    return {\n",
    "        'Accuracy': round(accuracy, 4),\n",
    "        'Recall': round(recall, 4),\n",
    "        'Precision': round(precision, 4),\n",
    "        'F1': round(f1, 4)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e2ad8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count true positives, false positives, and false negatives\n",
    "def count_true_positives(gt_df, pred_df, column='Allocation', join_on='Study', positive_value='Random'):\n",
    "    \"\"\"\n",
    "    Counts the number of true positives in the ground truth and predicted dataframes.\n",
    "    Args:\n",
    "        gt_df (pd.DataFrame): Ground truth dataframe.\n",
    "        pred_df (pd.DataFrame): Predicted dataframe.\n",
    "        column (str): The column name to check for true positives.\n",
    "        join_on (str): The column name to join the dataframes on.\n",
    "        positive_value (str): The value considered as a positive case.\n",
    "    Returns:\n",
    "        int: The count of true positives.\n",
    "    \"\"\"\n",
    "    merged = gt_df[[join_on, column]].merge(pred_df[[join_on, column]], on=join_on, suffixes=('_gt', '_pred'))\n",
    "    tp = ((merged[f\"{column}_gt\"] == positive_value) & (merged[f\"{column}_pred\"] == positive_value)).sum()\n",
    "    return tp\n",
    "\n",
    "def count_false_positives(gt_df, pred_df, column='Allocation', join_on='Study', positive_value='Random'):\n",
    "    \"\"\"Counts the number of false positives in the ground truth and predicted dataframes.\n",
    "    Args:\n",
    "        gt_df (pd.DataFrame): Ground truth dataframe.\n",
    "        pred_df (pd.DataFrame): Predicted dataframe.\n",
    "        column (str): The column name to check for false positives.\n",
    "        join_on (str): The column name to join the dataframes on.\n",
    "        positive_value (str): The value considered as a positive case.\n",
    "    Returns:\n",
    "        int: The count of false positives.\n",
    "    \"\"\"\n",
    "    merged = gt_df[[join_on, column]].merge(pred_df[[join_on, column]], on=join_on, suffixes=('_gt', '_pred'))\n",
    "    fp = ((merged[f\"{column}_gt\"] != positive_value) & (merged[f\"{column}_pred\"] == positive_value)).sum()\n",
    "    return fp\n",
    "\n",
    "def count_false_negatives(gt_df, pred_df, column= 'Allocation', join_on='Study', positive_value='Random'):\n",
    "    \"\"\"Counts the number of false negatives in the ground truth and predicted dataframes.\n",
    "    Args:\n",
    "        gt_df (pd.DataFrame): Ground truth dataframe.\n",
    "        pred_df (pd.DataFrame): Predicted dataframe.\n",
    "        column (str): The column name to check for false negatives.\n",
    "        join_on (str): The column name to join the dataframes on.\n",
    "        positive_value (str): The value considered as a positive case.\n",
    "    Returns:\n",
    "        int: The count of false negatives.\"\"\"\n",
    "    \n",
    "    merged = gt_df[[join_on, column]].merge(pred_df[[join_on, column]], on=join_on, suffixes=('_gt', '_pred'))\n",
    "    fn = ((merged[f\"{column}_gt\"] == positive_value) & (merged[f\"{column}_pred\"] != positive_value)).sum()\n",
    "    return fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4dbb23",
   "metadata": {},
   "source": [
    "### Testing the results :\n",
    "This will probably be deleted or reorgenised to keep the data in a better format for future plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81b971b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = count_true_positives(ground_truth_df, extracted_combined_df, column='Allocation', join_on='Study', positive_value='Random')\n",
    "print(f'True Positives: {tp}')\n",
    "\n",
    "fp = count_false_positives(ground_truth_df, extracted_combined_df, column='Allocation', join_on='Study', positive_value='Random')\n",
    "print(f'False Positives: {fp}')\n",
    "\n",
    "fn = count_false_negatives(ground_truth_df, extracted_combined_df, column='Allocation', join_on='Study', positive_value='Random')\n",
    "print(f'False Negatives: {fn}')\n",
    "\n",
    "# Calculate metrics\n",
    "allocation_metrics = calculateMetrics(tp, fp, fn)\n",
    "print(allocation_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e99188",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_teacher = count_true_positives(ground_truth_df, extracted_combined_df, column='Experimenter', join_on='Study', positive_value='Teacher')\n",
    "print(f'True Positives (Teacher): {tp_teacher}')\n",
    "\n",
    "fp_teacher = count_false_positives(ground_truth_df, extracted_combined_df, column='Experimenter', join_on='Study', positive_value='Teacher')\n",
    "print(f'False Positives (Teacher): {fp_teacher}')\n",
    "\n",
    "fn_teacher = count_false_negatives(ground_truth_df, extracted_combined_df, column='Experimenter', join_on='Study', positive_value='Teacher')\n",
    "print(f'False Negatives (Teacher): {fn_teacher}')\n",
    "\n",
    "# Calculate metrics for Teacher\n",
    "metrics_teacher = calculateMetrics(tp_teacher, fp_teacher, fn_teacher)\n",
    "print(metrics_teacher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e118f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_parent = count_true_positives(ground_truth_df, extracted_combined_df, column='Experimenter', join_on='Study', positive_value='Parent')\n",
    "print(f'True Positives (Parent): {tp_parent}')\n",
    "\n",
    "fp_parent = count_false_positives(ground_truth_df, extracted_combined_df, column='Experimenter', join_on='Study', positive_value='Parent')\n",
    "print(f'False Positives (Parent): {fp_parent}')\n",
    "\n",
    "fn_parent = count_false_negatives(ground_truth_df, extracted_combined_df, column='Experimenter', join_on='Study', positive_value='Parent')\n",
    "print(f'False Negatives (Parent): {fn_parent}')\n",
    "\n",
    "metrics_parent = calculateMetrics(tp_parent, fp_parent, fn_parent)\n",
    "print(metrics_parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f177913",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_researcher = count_true_positives(ground_truth_df, extracted_combined_df, column='Experimenter', join_on='Study', positive_value='Researcher')\n",
    "print(f'True Positives (Researcher): {tp_researcher}')\n",
    "\n",
    "fp_researcher = count_false_positives(ground_truth_df, extracted_combined_df, column='Experimenter', join_on='Study', positive_value='Researcher')\n",
    "print(f'False Positives (Researcher): {fp_researcher}')\n",
    "\n",
    "fn_researcher = count_false_negatives(ground_truth_df, extracted_combined_df, column='Experimenter', join_on='Study', positive_value='Researcher')\n",
    "print(f'False Negatives (Researcher): {fn_researcher}')\n",
    "\n",
    "metrics_researcher = calculateMetrics(tp_researcher, fp_researcher, fn_researcher)\n",
    "print(metrics_researcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4bbf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_Combined = count_true_positives(ground_truth_df, extracted_combined_df, column='Experimenter', join_on='Study', positive_value='Combined - Teacher and Parent')\n",
    "print(f'True Positives (Combined): {tp_Combined}')\n",
    "\n",
    "fp_Combined = count_false_positives(ground_truth_df, extracted_combined_df, column='Experimenter', join_on='Study', positive_value='Combined - Teacher and Parent')\n",
    "print(f'False Positives (Combined): {fp_Combined}')\n",
    "\n",
    "fn_Combined = count_false_negatives(ground_truth_df, extracted_combined_df, column='Experimenter', join_on='Study', positive_value='Combined - Teacher and Parent')\n",
    "print(f'False Negatives (Combined): {fn_Combined}')\n",
    "\n",
    "metrics_Combined = calculateMetrics(tp_Combined, fp_Combined, fn_Combined)\n",
    "print(metrics_Combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da53033c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_Experimenter = calculateMetrics(tp_teacher + tp_parent + tp_researcher + tp_Combined,\n",
    "                                        fp_teacher + fp_parent + fp_researcher + fp_Combined,\n",
    "                                        fn_teacher + fn_parent + fn_researcher + fn_Combined)\n",
    "print(\"Experimenter : \", metrics_Experimenter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35d7277",
   "metadata": {},
   "source": [
    "# Plots \n",
    "plotting the outputted numbers uwu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38f6ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34383033",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_plot_allocation_metrics(metrics_dicts, model_names):\n",
    "    \"\"\"\n",
    "    Plots allocation metrics (Accuracy, Recall, Precision, F1) for different LLMs.\n",
    "\n",
    "    Args:\n",
    "        metrics_dicts (list of dict): List of metrics dictionaries, one per model.\n",
    "        model_names (list of str): List of model names corresponding to metrics_dicts.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    labels = ['Accuracy', 'Recall', 'Precision', 'F1']\n",
    "    x = range(len(labels))\n",
    "    colors = ['hotpink', '#88c999', 'purple', \"#92e6df\",'#e15759', '#edc949', ]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    width = 0.8 / len(metrics_dicts)  # Adjust bar width based on number of models\n",
    "\n",
    "    for i, (metrics, name) in enumerate(zip(metrics_dicts, model_names)):\n",
    "        values = [metrics[label] for label in labels]\n",
    "        plt.bar([xi + i * width for xi in x], values, width=width, label=name, color=colors[i % len(colors)])\n",
    "\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Allocation and Experimenter metrics (ChatGPT output)')\n",
    "    plt.xticks([xi + width * (len(metrics_dicts) - 1) / 2 for xi in x], labels)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend()\n",
    "    #plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679ec21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_plot_allocation_metrics([allocation_metrics, metrics_Experimenter], ['Allocation', 'Experimenter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3961b941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test plot_allocation_metrics with a single model\n",
    "bar_plot_allocation_metrics([ metrics_parent, metrics_researcher, metrics_teacher, metrics_Combined], ['Parent', 'Researcher', 'Teacher', 'Combined'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecca342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_plot_accuracy_vs_time(accuracies, computing_times, model_names):\n",
    "    \"\"\"\n",
    "    Plots a scatter plot of accuracy vs computing time for each LLM.\n",
    "\n",
    "    Args:\n",
    "        accuracies (list): List of accuracy values for each model.\n",
    "        computing_times (list): List of computing times for each model.\n",
    "        model_names (list): List of model names (e.g., ['chatgpt', 'llama', 't5']).\n",
    "    \"\"\"\n",
    "\n",
    "    default_colors = ['hotpink', '#88c999', 'purple', 'blue', \"#92e6df\", '#e15759', '#edc949']\n",
    "    colors = default_colors * ((len(accuracies) // len(default_colors)) + 1)\n",
    "    colors = colors[:len(accuracies)]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(computing_times, accuracies, color= colors, s=100)\n",
    "\n",
    "    for i, name in enumerate(model_names):\n",
    "        plt.annotate(name, (computing_times[i], accuracies[i]), textcoords=\"offset points\", xytext=(5,5), ha='left')\n",
    "\n",
    "    plt.xlabel('Computing Time (s)')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy vs Computing Time for LLMs')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b021f520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the scatter_plot_accuracy_vs_time function with available data\n",
    "scatter_plot_accuracy_vs_time([0.3, 0.4, 0.896, 1], [5, 3, 0.5, 10], ['model_names', 'gpt', 'llama', 'test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b2df65",
   "metadata": {},
   "source": [
    "# To do :\n",
    "nexts steps are to add graphics\n",
    "- multiple subplots on a single page side by side"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
